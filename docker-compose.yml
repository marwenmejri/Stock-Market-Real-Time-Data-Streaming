version: '3.8'

services:
  # MinIO - Object storage
  minio:
    image: minio/minio:latest
    container_name: minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"

  # PostgreSQL - Relational database
  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      POSTGRES_DB: airflow
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  # Kafka Brokers
  kafka1:
    image: confluentinc/cp-kafka:latest
    container_name: kafka1
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168      # 7 days
      KAFKA_LOG_RETENTION_BYTES: -1       # Unlimited by size
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB per segment
    ports:
      - "9092:9092"
    volumes:
      - kafka1_data:/var/lib/kafka/data

  kafka2:
    image: confluentinc/cp-kafka:latest
    container_name: kafka2
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168      # 7 days
      KAFKA_LOG_RETENTION_BYTES: -1       # Unlimited by size
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB per segment
    ports:
      - "9093:9093"
    volumes:
      - kafka2_data:/var/lib/kafka/data

  kafka3:
    image: confluentinc/cp-kafka:latest
    container_name: kafka3
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168      # 7 days
      KAFKA_LOG_RETENTION_BYTES: -1       # Unlimited by size
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB per segment
    ports:
      - "9094:9094"
    volumes:
      - kafka3_data:/var/lib/kafka/data

  # Kafka Topic Creator (runs once)
  kafka-init:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-init
    depends_on:
      - kafka1
    entrypoint: [ "bash", "-c", "sleep 20 && kafka-topics --create --if-not-exists --bootstrap-server kafka1:9092,kafka2:9093,kafka3:9094 --replication-factor 3 --partitions 3 --topic stock-market" ]

  # Spark Master
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./scripts:/opt/bitnami/spark/scripts
      - spark_history:/opt/bitnami/spark/history
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./extra-jars:/opt/bitnami/spark/extra-jars

  # Spark Workers
  spark-worker1:
    image: bitnami/spark:latest
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - spark_history:/opt/bitnami/spark/history
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./extra-jars:/opt/bitnami/spark/extra-jars

  spark-worker2:
    image: bitnami/spark:latest
    container_name: spark-worker2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - spark_history:/opt/bitnami/spark/history
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./extra-jars:/opt/bitnami/spark/extra-jars

  spark-worker3:
    image: bitnami/spark:latest
    container_name: spark-worker3
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - spark_history:/opt/bitnami/spark/history
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./extra-jars:/opt/bitnami/spark/extra-jars

  # Spark History Server
  spark-history-server:
    user: root
    image: bitnami/spark:latest
    container_name: spark-history-server
    ports:
      - "18080:18080"
    volumes:
      - spark_history:/opt/bitnami/spark/history
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
      - ./extra-jars:/opt/bitnami/spark/extra-jars
    entrypoint: >
      bash -c "/opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer --properties-file /opt/bitnami/spark/conf/spark-defaults.conf"

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.9.1
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin123@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__WEBSERVER__SECRET_KEY: a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
    ports:
      - "8081:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --firstname Marwen --lastname User --role Admin --password admin --email admin@example.com &&
      airflow webserver"

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.9.1
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin123@postgres:5432/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
    command: airflow scheduler

  # Stock Ingestor
  stock-ingestor:
    build:
      context: .
      dockerfile: Dockerfile.ingestor
    container_name: stock-ingestor
    volumes:
      - ./scripts:/app/scripts
      - ./config:/app/config
    tty: true

volumes:
  minio_data:
  postgres_data:
  kafka1_data:
  kafka2_data:
  kafka3_data:
  spark_history:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
